---
title: Exploring probability distributions for bivariate temporal granularities
authors:
- name: Sayani Gupta
  affiliation: Department of Econometrics and Business Statistics, Monash University
  email: Sayani.Gupta@monash.edu

- name: Rob J Hyndman
  affiliation: Department of Econometrics and Business Statistics, Monash University

- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University

keywords:
- data visualization
- statistical distributions
- time granularities
- periodicties
- grammar of graphics
- R

abstract: |
 Recent advances in technology greatly facilitates recording and storing data at much finer temporal scales than was previously possible. As the frequency of time-oriented data increases, the number of questions about the observed variable that need to be addressed by visual representation also increases. We propose some new tools to explore this type of data, which deconstruct time in many different ways. There are several classes of time deconstructions including linear time granularities, circular time granularities and aperiodic calendar categorizations. Linear time granularities respect the linear progression of time such as hours, days, weeks and months. Circular time granularities accommodate periodicities in time such as hour of the day, and day of the week. Aperiodic calendar categorizations are neither linear nor circular, such as day of the month or public holidays.

  The hierarchical structure of linear granularities creates a natural nested ordering resulting in single-order-up and multiple-order-up granularities. For example, hour of the week and second of the hour are both multiple-order-up, while hour of the day and second of the minute are single-order-up.

   Visualizing data across granularities which are either single-order-up or multiple-order-up or periodic/aperiodic helps us to understand periodicities, pattern and anomalies in the data. Because of the large volume of data available, using displays of probability distributions conditional on one or more granularities is a potentially useful approach. This work provides tools for creating granularities and exploring the associated time series within the tidy workflow, so that probability distributions can be examined using the range of graphics available in [@Wickham2009-pk]. 
bibliography: bibliography.bib
output:
  bookdown::pdf_book:
    #base_format: rticles::asa_article
    fig_height: 5
    fig_width: 8
    fig_caption: yes
    dev: "pdf"
---

```{r initial, echo = FALSE, cache = FALSE, include = FALSE}
options("knitr.graphics.auto_pdf" = TRUE)
library(knitr)
library(tidyverse)
library(lubridate)
library(lvplot)
library(ggridges)
library(viridis)
library(tsibble)
library(gravitas)
opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE, comment = "#>",
  fig.path = "figure/", fig.align = "center", fig.show = "hold",
  cache = FALSE, cache.path = "cache/",
  out.width = ifelse(is_html_output(), "100%", "\\textwidth")
)
knitr::opts_knit$set(root.dir = here::here())
```

```{r external, include = FALSE}
# read_chunk('scripts/main.R')
```

```{r load}

```

# Introduction

<!--temporal granularities and why should we care --> 
Temporal data can be available at various resolution depending on the context. Social and economic data are often collected and reported at coarser temporal scales like monthly, quarterly or annually. But with recent advancement in technology, more and more data are recorded and stored at much finer temporal scales than that was previously possible. It might be sufficient to observe energy consumption every half an hour, but energy supply needs to be monitored every minute and number of web searches requires optimization every second.  As the frequency of data increases, the number of questions about the observed variable that need to be addressed also increases. For example, data collected at an hourly scale can be analyzed using coarser temporal scales like days, months or quarters. This approach requires deconstructing time in various possible ways. 

A temporal granularity which results from such a deconstruction may be intuitively described as a sequence of time granules, each one consisting of a set of time instants. There are several classes of time deconstructions including linear time granularities, circular time granularities and aperiodic calendar categorizations. Linear time granularities respect the linear progression of time such as hours, days, weeks and months. Circular time granularities accommodate periodicities in time such as hour of the day, and day of the week. Aperiodic calendar categorizations are neither linear nor circular, such as day of the month or public holidays.

It is important to be able to navigate through all of these temporal granularities to have multiple perspectives on the observed data. This idea aligns with the notion of EDA [@Tukey1977-jx] which emphasizes the use of multiple perspectives on data to help formulate hypotheses before proceeding to hypothesis testing. 


<!--motivation --> 
   
The motivation for this work comes from the desire to provide methods to better understand large quantities of measurements on energy usage reported by smart meters in household across Australia, and indeed many parts of the world. Smart meters currently provide half-hourly use in kWh for each household, from the time that they were installed, some as early as 2012. Households are distributed geographically, and have different demographic properties such as the existence of solar panels, central heating or air conditioning. The behavioral patterns in households vary substantially, for example, some families use a dryer for their clothes while others hang them on a line, and some households might consist of night owls, while others are morning larks. 

It is common to see aggregates of usage across households, total kWh used each half hour by state, for example, because energy companies need to understand maximum loads that they will have to plan ahead to accommodate. But studying overall energy use hides the distributions of usage at finer scales, and making it more difficult to find solutions to improve energy efficiency. 

We propose that the analysis of probability distributions of smart meter data at finer or coarser scales can be benefited from the approach of Exploratory Data Analysis (EDA). EDA calls for utilizing visualization and transformation to explore data systematically. It is a process of generating hypothesis, testing them and consequently refining them through investigations.

<!--details abt time gran --> 
The hierarchical structure of many granularities creates a natural nested ordering. For example, hours are nested within days, days within weeks, weeks within months, and so on. We refer to granularities which are nested within multiple levels as "multiple-order-up" granularities. For example, hour of the week and second of the hour are both multiple-order-up, while hour of the day and second of the minute are single-order-up. 

<!--existing closely related work and how ours relate --> 
 This paper utilizes the nestedness of time granularities to obtain multiple-order-up granularities from single-order-up ones.

<!--actions on multiple order up - usage --> 
Finally, visualizing data across single/multiple order-up granularities help us to understand periodicities, pattern and anomalies in the data. Because of the large volume of data available, using displays of probability distributions conditional on one or more granularities is a potentially useful approach. However, this approach can lead to a myriad of choices all of which are not useful. Analysts are expected to iteratively visualize these choices for exploring possible patterns in the data. But too many choices might leave him bewildered. 

<!-- challenges and how you are dealing wih it --> 
This work provides tools for systematically exploring bivariate granularities within the tidy workflow through proper study of what can be considered a prospective graphic for exploration. Pairs of granularities are categorized as either a *harmony* or *clash*, where harmonies are pairs of granularities that aid exploratory data analysis, and clashes are pairs that are incompatible with each other for exploratory analysis. Probability distributions can be examined using the range of graphics available in the [ggplot2](https://cran.r-project.org/package=ggplot2) package. 


In particular, this work provides the following tools.

  * Functions to create multiple-order-up time granularities. This is an extension to the [lubridate]([https://cran.r-project.org/package=lubridate) package, which allows for the creation of some calendar categorizations, usually single-order-up.

  * Checks on the feasibility of creating plots or drawing inferences from two granularities together. Pairs of granularities can be categorized as either a *harmony* or *clash*, where harmonies are pairs of granularities that aid exploratory data analysis, and clashes are pairs that are incompatible with each other for exploratory analysis.

<!-- The remainder of the paper is organized as follows. Section \@ref(sec:algorithm) details the construction of the calendar layout in depth. It describes the algorithms of data transformation (Section \@ref(sec:transformation)), the available options (Section \@ref(sec:opt)), and variations of its usage (Section \@ref(sec:variations)). Section \@ref(sec:facet-calendar) explains the full faceting extension, that is equipped with formal labels and axes. An analysis of half-hourly household energy consumption, using the calendar display, is illustrated in a case study in Section \@ref(sec:case). Section \@ref(sec:discussion) discusses the limitations of calendar displays and possible new directions. -->

# Formal conceptualization of calendar categorizations{#cirgran-def}

<!-- general calendar categorization -->
Often we partition time into months, weeks, days and so on in a hierarchical manner to relate it to data. Such discrete abstractions of time can be thought of as time granularities [@aigner2011visualization]. Examples of time abstractions may also include day-of-week, time-of-day, week-of-year, day-of-month, month-of-year, working day/non-working day, etc which are useful to represent different periodicities in the data. Let us call all of these different abstractions of time as "calendar categorizations".

These calendar categorizations can be linear, circular or aperiodic. The calendar categorizations as "linear" if they respect the linear progression of time. We call these **linear time granularities**. Examples include hours, days, weeks and months. **Circular time granularities** accommodate periodicities in time such as hour of the day, and day of the week. **Aperiodic time granularities** are neither linear nor circular, such as day of the month or public holidays.

<!-- <!-- Representation with tsibble -->
<!-- Suppose we have a tsibble with a time index in one column and keys and variables in other columns. A time domain, as defined by [@Bettini1998-ed], is essentially a mapping of row numbers (the index set) to the time index. A linear granularity is a mapping of row numbers to subsets of the time domain. For example, if the time index is days, then a linear granularity might be weeks, months or years. -->

<!-- <!-- What is calendar categorization --> 
<!-- What we need to add to this are additional categorizations of time that are not linear granularities and are useful to represent periodicity. Examples include day-of-week, time-of-day, week-of-year, day-of-month, month-of-year, working day/non-working day, etc. Many of these are circular, such as day-of-week, time-of-day. Some are nearly circular such as day-of-month. Some are irregular such as working day/non-working day. Let's call all of these "calendar categorizations". Anything that maps a time index to a categorical variable can be considered a **calendar categorization**.  -->

<!-- What is circular granularity and some examples -->
<!-- We specify the circular categorizations using modular arithmetic and call these **circular granularities**. The number of categories is essentially the periodicity of a circular time granularity. For example, suppose the time index is in minutes, and let $n_i$ be the number of categories created by the circular granularity $C_i$. Then the following categorizations can be computed. -->

Providing a formalism to these abstractions is important to model a time series across differently grained temporal domains.

## Linear time granularities{#linear-gran-def}

There has been several attempts to provide the framework for formally characterizing time-granularities and identifying their structural properties, relationships and symbolic representations. One of the first attempts occur in [@Bettini1998-ed] with the help of the following definitions: 

**Definition:** A **time domain** is a pair $(T; \le)$ where $T$ is a non-empty set of time instants and $\le$ is a total order on $T$.

A time domain can be **discrete** (if there is unique predecessor and successor for every element except for the first and last one in the time domain), or it can be **dense** (if it is an infinite set). A time domain is assumed to be discrete for the purpose of our discussion.

**Definition:** A linear **granularity** is a mapping $G$ from the integers (the index set) to subsets of the time domain such that:

  (C1) if $i < j$ and $G(i)$ and $G(j)$ are non-empty, then each element of $G(i)$ is less
than all elements of $G(j)$, and  
  (C2) if $i < k < j$ and $G(i)$ and $G(j)$ are non-empty, then $G(k)$ is non-empty.  

**Definition:** Each non-empty subset $G(i)$ is called a **granule**, where $i$ is one of the indexes and $G$ is a linear granularity.

The first condition implies that the granules in a linear granularity are non-overlapping and their index order is same as time order. \autoref{fig:linear-time} shows the implication of this condition. If we consider the bottom linear granularity [@aigner2011visualization] as hourly and the entire horizon has T hours then it will have $\lfloor T/24\rfloor$ days, $\lfloor s/(24*7)\rfloor$ weeks and so on.


```{r linear-time,echo=FALSE,out.width = "100%",fig.cap="\\label{fig:linear-time} The time domain distributed as linear granularities"}
knitr::include_graphics("Figs/linear-time.png")
```

The definitions and rules for linear granularities are inadequate to reflect periodicities in time, like weekly, monthly or yearly seasonality. Hence, there is a need to define circular time granularities in a different approach. 

## Circular time granularities{#circular-gran-def}

A time domain, as defined by [@Bettini1998-ed], is essentially a mapping of row numbers (the index set) to the time index in a tsibble[@Wang2019-aw] for a given key. A linear granularity is a mapping of row numbers to subsets of the time domain. For example, if the time index is days, then a linear granularity might be weeks, months or years. What we need to add to this are additional categorizations of time that are not linear granularities and are useful to represent periodicity.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lr@{~}lr@{~}r}
\toprule
MOH: & $C_1(s)$ & $= s \mod 60$ & $n_1 =$&$60$ \\
MOD: & $C_2(s)$ & $= s \mod `r 60*24`$ & $n_2=$&$`r 60*24`$\\

HOD: & $C_4(s)$ & $= \lfloor s/60\rfloor\mod 24$  & $n_4 =$&$24$ \\
HOW: & $C_6(s)$ & $= \lfloor s/60\rfloor\mod 24*7$  & $n_5=$&$`r 24*7`$\\

DOW: & $C_7(s)$ & $= \lfloor s/24*60\rfloor \mod 7$ & $n_6=$&$7$\\
\bottomrule
\end{tabular}
\end{center}
\caption{Illustrative circular granularities with time index in minutes}
\label{definitions}
\end{table}

<!-- It is easy to note here that due to unequal length of some linear granularities like months or years, these formulae can not be used for computing week-of-month or day-of-year. Thus, we start with the definition of circular granularity and then move on to computing aperiodic granularities.  -->

We want to use modular arithmetic to define circular granularity. Hence, we start with the definition of equivalence classes and then move on to define a circular granularity. 

<!-- Formal definition of circular granularity -->
**Definition: Equivalence class** Let $m \in N \backslash{0}$. For any $a \in Z$ (set of integers), $[a]$ is defined as the equivalence class to which a belongs if $[a]$ = {$b \in Z | a \equiv (b \mod m)$}.

The set of all equivalence classes of the integers for a modulus $m$ is called the ring of integers modulo $m$, denoted by $Z_m$. Thus $Z_m = \{[0], [1], ..., [m-1]\}$. However, we often write $Z_m = \{0, 1, ..., (m-1)\}$, which is the set of integers modulo $m$.

**Definition:** A **circular granularity** $C$ with a modular period m is defined to be a mapping from the integers $Z$ (Index Set) to $Z_m$, such that $C(s) = (s\mod m)$ for $s \in Z$.

For example, suppose $C$ is a circular granularity denoting Hour-of-Day and we have hourly data for 100 hours. The modular period $m = 24$, since each day consists of 24 hours and $C$ is a mapping from ${1,2,\dots, 100}$ to ${0,1,2, \dots, 23}$ such that  $C(s)= s \mod 24$ for $s \in {1,2,\dots, 100}$.

**Definition:** A **cycle** is defined as the progression of each circular granularity with modular period m through {$1,2,\dots,(m-1),0$} once.

**Definition:** A **circular granule** represents an equivalence class inside each cycle.

## Aperiodic time granularities{#aperiodic-gran-def}
<!-- Formal definition of aperiodic circular granularity -->

**Definition:** An **Aperiodic circular granularity** can not be defined using modular arithmetic in a similar fashion. The modulus for these type of calendar categorizations are not constant due to unequal length of some linear granularities. For example, please refer to the table below:


\begin{table}[ht]
\begin{center}
\begin{tabular}{lr@{~}lr@{~}r}
\toprule

HOM: & $C_3(s)$ & $= s \mod `r 24*30`$ (approximately) & $n_3=$&$`r 24*31`$\\
HOY: & $C_4(s)$ & $= s \mod `r 24*365`$ (except for leap years) & $n_4=$&$`r 24*366`$\\
DOM: & $C_6(s)$ & $= \lfloor s/24\rfloor\mod 30$  (approximately) & $n_6=$&$31$\\
DOY: & $C_7(s)$ & $= \lfloor s/24\rfloor \mod 365$ (except for leap years) & $n_7=$&$366$\\
WOM: & $C_8(s)$ & $=\lfloor s/168 \rfloor \mod 4$ (approximately) & $n_8 =$&$ 5$\\
WOY: & $C_9(s)$ & $=\lfloor s/168\rfloor \mod 52$ (approximately) & $n_9 =$&$ 53$\\
MOY: & $C_{10}(s)$ & $= \lfloor s/720\rfloor \mod 12$ (approximately) & $n_{10}=$&$12$\\
\bottomrule
\end{tabular}
\end{center}
\caption{Illustrative aperiodic circular granularities with time index in hours}
\label{definitions}
\end{table}

 <!-- Why we need single order up and multiple order up granularities -->
<!-- A time series which is characterized by being composed of repeating cycles. -->


Identifying repeating (periodic/aperiodic) patterns are necessary in revealing patterns and future trends of a temporal data. Often there is a need for periodicity detection to find whether and how frequent a periodic/aperiodic pattern is repeated within the series. To consider the exhaustive set of temporal regularities that might exist in the data, we can categorize the set of temporal granularities to single or multiple order up granularities.

# Single-order-up and multiple-order-up granularities


<!-- some discussions on primary units of calendar -->
So far we have used the Gregorian calendar as it is the most widely used calendar. But it is far from being the only one. All calendars fall under three types - solar, lunar or lunisolar/solilunar but the day is the basic unit of time underlying all calendars. Various calendars, however, use different conventions to structure days into larger units: weeks, months, years and cycle of years. Any civil day is divided into 24 hours and each hour into 60 minutes and each minute into 60 seconds. There are exceptions where in London, for example, length of each "hour" varies from about 39 minutes in December to 83 minutes in June. The French revolutionary calendar divided each day into 10 "hours", each "hour" into 100 "minutes" and each "minute" into 100 "seconds". Nevertheless, for any calendar, the hierarchical structure of time creates a natural nested ordering which can produce **single-order-up** or **multiple-order-up** granularities. We shall use the notion of a hierarchy table to define them.

Consider a **hierarchy table** to be consisting of two columns:  
 - The first column represents the (temporal) units in ascending order of hierarchy.  
 - The second column represents the constant which relates subsequent (temporal) units.  
   
and, **order**, is defined as the position of the units in the hierarchical table. 

We refer to granularities which are nested within multiple levels as **multiple-order-up** granularities and those concering a single level as **single-order-up** granularities. "Mayan" calendar is used to illustrate this. In Mayan calendar, one day was referred to as 1 kin and the calendar was structured as follows: 

 - 1 kin = 1 day
 - 1 uinal = 20 kin
 - 1 tun  = 18 uinal
 - 1 katun = 20 tun
 - 1 baktun = 20 katun

Thus, the hierarchy table for the Mayan calendar would look like the following:

| Units	      | Conversion factor
|------------	|-------------------	|
|  kin      	| 20             	    |
|  uinal    	| 18            	    |
|  tun      	| 20           	      |
|  katun    	| 20            	    |
|  baktun   	| 1          	        |

Examples of multiple-order-up  granularities can be kin of the tun or kin of the baktun whereas examples of single-order-up granularities may include kin of the uinal, uinal of the tun etc. 

In the next section, we discuss the computation of any single-order-up and multiple-order-up granularities in a periodic and aperiodic set up.

## Computation of single-order-up granularities

Suppose, z denotes the index of the tsibble, x,y are two units in the hierarchy table  with  $order(x) <  order(y)$. Let $f(x, y)$ denotes the accessor function for computing the single-order-up granularity x-of-y and $c(x, y)$ is a constant which relates x and y.

<!-- We consider a tsibble with an index (I), key(optional) and measured variables. Index is the variable with inherent ordering from past to future. -->

Then $f(x, y)$ can be  computed using modular arithmetic as follows:

$$f(x, y) = \lfloor z/c(z,x) \rfloor\mod c(x,y)$$ where $y = x+1$

See table[] for an illustration of computing single order up granularities for Mayan calendar.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lr@{~}lr@{~}r}
\toprule
\textbf{Single-order-up granularities}


$kin\_uinal:   = z \mod 20$  \\
$uinal\_tun:   = \lfloor z/20\rfloor \mod 18 $\\
$tun\_katun:    = \lfloor z/20*18\rfloor \mod 20 $\\
$katun\_baktun:   = \lfloor z/20*18*20\rfloor \mod 20 $\\

\bottomrule
\end{tabular}
\end{center}
\caption{Illustrative single-order-up granularities for Mayan calendar with kin as the index}
\label{single_order}
\end{table}

### Aperiodic single-order-up granularities

Aperiodic single-order-up granularities refers to the granularities which are formed with units that does not repeat its values in regular intervals or periods. In Gregorian calendar, examples may include days of the month where each month may consists of 28, 29, 30 or 31 days. So there is no single number that can be used for converting days to months. We cannot compute the aperiodic single-order-up granularities using the index of the tsibble and modular arithmetic like in the periodic case.  


## Computation of multiple order-up granularities 

Computation of multiple-order-up granularities will differ if the units in the hierarchy table are periodic or aperiodic. We split the method of computation into two cases - one concerining all periodic single-order-up granularities and the second with mixed single-order-up granularities.

### All single-order-up granularities are periodic


z is the index set of the tsibble, x,y are two linear granularities with  $order(x) <  order(y)$. Also, $f(x, y)$ denotes the accessor function for computing circular granularity x_y and $c(x, y)$ is a constant which relates x and y. It is easy to see that for $order(x+1) =  order(y)$, the function is same as the single-order-up granularities.

Then, the accessor function f can be used recursively to obtain any multiple-order-up granularities as follows:

\begin{equation} \label{eq1}
\begin{split}
f(x,y) & = f(x, x+1) + c(x, x+1)(f(x+1, y)-1) \\  
  & =  f(x, x+1) + c(x, x+1)[f(x+1, x+2) + c(x+1, x+2)(f(x+2, y) - 1) - 1]\\
  & =  f(x, x+1) + c(x, x+1)(f(x+1, x+2) - 1) + c(x, x+1)c(x+1, x+2)(f(x+2, y) - 1)\\
  & =  f(x, x+1) + c(x, x+1)(f(x+1, x+2) - 1) + c(x, x+2)(f(x+2, y) - 1)\\
  &\vdots\\
  & = \sum_{i=0}^{order(y) - order(x) - 1} c(x, x+i)(f(x
  +i, x+i+1) - 1)\\
\end{split}
\end{equation}

Let us use the equation to compute the multiple-order-up granularity uinal_katun for Mayan calendar, which is periodic.

From Equation(1), we have
\begin{equation} \label{eq1}
\begin{split}
f(uinal, baktun) & = f(uinal, tun) + c(uinal, tun)f(tun,katun) + c(uinal, katun)f(katun, baktun) \\
              & = \lfloor z/20\rfloor \mod 18  + 20*\lfloor z/20*18\rfloor \mod 20 + 20*18*20\lfloor z/20*18*20\rfloor \mod 20 \\
\end{split}
\end{equation}
             (using equations \ref{single_order})

### Mixed single-order-up granularities - circular or aperiodic
      
Let us turn to Gregorian calendar for addressing this case.

Suppose we have a hierarchy table for  Gregorian calendar as \ref{table_accessor2}. Since months consists of unequal number of days, any temporal unit which is higher in order than months will also have unequal number of days. This is an example of a hierarchy table which has both periodic and aperiodic single-order-up granularities. The single-order-up granularity day_month is aperiodic. Any single-order-up granularities which are formed by units below days are periodic. Similarly, all single-order-up granularities which are formed using units whose orders are higher than months are also periodic.


<!-- Hence, any calendar categorization which includes one linear granularity with orders at most as days and another one with orders at least as months will be aperiodic in nature. However, it is very much possible for a calendar categorization consisting of two linear granularities higher in order than month to be periodic.For example, the categorization month_quarter would be periodic since each quarter always consists of three months. -->

```{r table_accessor2}
kable(tibble::tibble(
  `linear granularities` = c("minute", "hour", "day", "month", "quarter", "year"),
  `Conversion factor` = c(60,24, "aperiodic", 3, 4, 1), format = "markdown")
```

There can be three scenarios for obtaining calendar categorization here:
- granularities consisting of two units whose orders are less than day  
- granularities consisting of two units whose orders are more than month  
- granularities consisting of one unit with order at most day and another with order at least month  

The calendar categorization resulting from the first two cases are periodic and has been handled in the earlier section.

The calendar categorization resulting from the last case are aperiodic. Examples might include day of the quarter or hour of the month. In this section, we will see how to obtain aperiodic circular granularities of these types.


\begin{equation} \label{eq1}
\begin{split}
f_{(hour, month)}(z) = f_{(hour, day)}(z) + c(hour, day)f_{(day, month)}(z)\\
\end{split}
\end{equation}

Here, the first part of the equation is a single-order-up granularity which can be obtained using last section. The second part is not single-order-up and can not be broken down further since each month consists of different number of days. In this case, it is important for us to know which month of the year and if the year is a leap year to obtain day of the month.
             

  
# Visualization

<!-- Why visualize -->
Analysts often want to fit their data to statistical models, either to test hypotheses or predict future values. However, improper choice of models can lead to wrong predictions. One important use of visualization is exploratory data analysis, which is gaining insight into how data is distributed to inform data transformation and modeling decisions.

<!-- When the data for entire population is available at our disposal, then there is no inference that is to be made for the unobserved population. We can just examine what is in front of us. -->


<!-- Why distribution plot -->
<!-- Huge data difficult to summaries using one statistic-->
But with huge amount of data being available, sometimes mean, median or any one summary statistic is not enough to understand a data set. Soon enough following questions become more interesting:  

 - Are values clustered around mean/median or mostly around tails? In other words, what is the combined weight of tails relative to the rest of the distribution? 
 
 - Does values rise very quickly between 25th percentile and median but not as quickly between median and 75th percentile? More generally, how the variation is the data set changes across different percentiles/deciles?
 
  - Is the tail on the left hand side longer than that on the right side? Or are they equally balanced around mean/median?
 
This is when displaying a probability distribution becomes a potentially useful approach.


<!-- What all can you see from a distribution plot-->

The entire distribution can be visualized or some contextual summary statistics can be visualized to emphasize certain properties of the distribution. These properties can throw light on central tendency, skewness, kurtosis, variation of the distribution and can also be useful in detecting extreme behavior or anomalies in the data set.


<!-- Lit review on statistical distribution plot-->
## Statistical distribution plots

Most commonly used techniques to display distribution of data include the histogram (Karl Pearson), which shows the prevalence of values grouped into bins and the box-and-whisker plots [@Tukey1977-jx] which convey statistical features such as the median, quartile boundaries, hinges, whiskers and extreme outliers. The box plot is a compact distributional summary, displaying less detail than a histogram. Due to wide spread popularity and simplicity in implementation, a number of variations are proposed to the original one which provides alternate definitions of quantiles, whiskers, fences and outliers. Notched box plots [@Mcgill1978-hg, 1978] has box widths proportional to the number of points in the group and display confidence interval around medians aims to overcome some drawbacks of box plots.

The vase plot [@Benjamini1988-io, 1988] was a major revision from the concept of box plots where the width of box at each point is proportional to estimated density. Violin plots [@Hintze1998-zi, 1998] display the density for all data points and not only the box. The summary plot [@Potter2010-qc, 2010] combines a minimal box plot with glyphs representing the first five moments (mean, standard deviation, skewness, kurtosis and tailings), and a sectioned density plot crossed with a violin plot (both color and width are mapped to estimated density), and an overlay of a reference distribution. The highest density region (HDR) box plot proposed by [@Hyndman1996-ft] displays a probability density region that contains points of relatively highest density. The probabilities for which the summarization is required can be chosen based on the requirement. These regions do not need to be contiguous and help identify multi-modality. The letter-value box plot [@Hofmann2017-sg, 2006] was designed to adjust for number of outliers proportional to the data size and display more reliable estimates of tail. Because this display just adds extra letter values, it suffers from the same problems as the original box plot, and multimodality is almost impossible to spot[@Wickham_undated-vr].

Moreover, much like the quartiles divide the data set equally into four equal parts, extensions might include dividing the data set even further. The deciles plots consist of 9 values that split the data set into ten parts and the percentile plot consists of 99 values that split the data set into hundred parts. A large data set is required before the extreme percentiles can be estimated with any accuracy. 

Finally, a density plot which uses a kernel density estimate to show the probability density function of the variable can show the entire distribution. Also, a Ridge line plot (sometimes called Joy plot) shows the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.




## Harmony and Clashes

We investigate some combinations of circular time granularities (periodic/aperiodic) which facilitate or hinder exploratory analysis. The combinations of circular granularities which promote the exploratory analysis through visualization are referred to as **harmonies** and the ones which impede the analysis are referred to as **clashes**.

<!-- Let's take a specific example, where $C_1$ maps row numbers to Day-of-Month and $C_2$ maps row numbers to Week-of-Month. Here $C_1$ can take 31 values while $C_2$ can take 5 values. There will be $31\times 5=155$ sets $S_{ij}$ corresponding to the possible combinations of WOM and DOM. Many of these are empty. For example $S_{1,5}$, $S_{21,2}$, etc. In fact, most of these 155 sets will be empty, making the combination of $C_1$ and $C_2$ in a graph unhelpful. These are structurally empty sets in that it is impossible for them to have any observations. -->

<!-- Another example could be where $C_1$ maps row numbers to Day-of-Week and $C_2$ maps row numbers to Month-of-Year. Here $C_1$ can take 7 values while $C_2$ can take 12 values. So there are $12\times7=84$ sets $S_{ij}$ corresponding to the possible combinations of DOW and MOY. All of these are non-empty because every DOW can occur in every month. So graphics involving $C_1$ and $C_2$ are potentially useful. -->


\autoref{fig:allFig} (a) shows the letter value plot of electricity consumption of Victoria across days of the month for few months like January, February, April and December. Letter value plots convey detailed information about tails of the distribution and outliers are unexpected observations rather than extreme observations. M, F, E, D and C represents 50%, 25%, 12.5%, 6.25% and 3.13% of the tail area respectively. Some observations that can be made from these letter-value plots include a) Right tails for most days in January lying above the median is more extended than the left tails contrary to days in April which has longer left tails. b) Days in mid January and February are characterized by high variation in consumption level, but days in January have longer right tails. c) Last five days in December (Christmas holidays) shows low variation in consumption with longer left tails implying people typically consume less electricity. We can conclude that Month-of-Year and Day-of-Month are harmonies.


\autoref{fig:allFig} (c) shows box plot of electricity consumption of Victoria from 2012 to 2014 across days of the year by the 1st, 15th, 29th and 31st days of the month. The box plot is a a very compact distributional summary, displaying median, quartile boundaries, hinges, whiskers and outliers. All facets do not contain data across same x-axis levels leading to difficulty in comparison across facets. Hence, Day-of-Month and Day-of-Year are clashes.

Take another example \autoref{fig:allFig} (d) showing violin plot across days of the month faceted by week of the month. Violin plots are a combination of box plot and density plot. Here, the first week of the month correspond to certain days of the month which are different for different week of the month. These kinds of graphics can hinder our comprehension across different weeks of the month and can be categorized as a incompatible combination to plot.

In \autoref{fig:allFig} (e), variations across week of the year conditional on week of the month can be observed through a ridge plot. The y-axis represents week of the year and the x-axis represents electricity consumption. Ridge plots are density plots all aligned to the same horizontal scale and presented with a slight overlap. They are designed to bring out changes in distributions over time. The data is distributed unequally for different levels of weeks of the year for different facets, which hinders comparison across facets. So, Week-of-Month and Week-of-Year are clashes.

\autoref{fig:allFig} (f) shows decile plots of consumption across  day of the year and month of the year. Decile plots are useful in displaying distribution through deciles without much clutter. This plot, however, seem ineffective for comparison as levels of x-axis for which observed data is plotted are completely disjoint across facets. So Month-of-Year and Day-of-Year are clashes as well.


From the above examples, we can see that the choices of the circular granularities are harmonies while some are clashes. Also since some choices work and others don't, it must be the attributes of the circular granularities or their relationships which are in play in deciding if the resulting plot would be a good candidate for exploratory analysis.


```{r moy-dom,echo=FALSE, out.width = "70%", fig.align = 'center',fig.cap ="Letter-value plot of electricity consumption in Victoria across days of the month\n faceted by months January, February, April and December. "}

# change all of these to gravitas function
VIC <- read.csv("data/VIC2015/Data/demand_VIC.csv")
VIC$Date <- as.Date(VIC$Date, origin = "1899-12-30")

first_day_of_month_wday <- function(dx) {
  day(dx) <- 1
  wday(dx)
}

VIC <- VIC %>% mutate(
  Indx_Year = year(Date),
  Indx_Month = month(Date, label = FALSE, abbr = TRUE),
  Indx_Wk_Yr = week(Date),
  Indx_Wk_Month = ceiling((day(Date) + first_day_of_month_wday(Date) - 1) / 7),
  Indx_Day_Week = wday(Date,
    label = FALSE, abbr = TRUE,
    week_start = 1
  ),

  Indx_Day_Month = day(Date),

  Indx_Day_Year = yday(Date),

  Indx_Weekend = if_else(Indx_Day_Week %in% c(6, 7), 1, 0),

  Indx_HlHr_day = Period,
  month = month(Date, label = FALSE, abbr = TRUE),
  year = year(Date),
  yday = yday(Date),
  wday = wday(Date,
    label = FALSE, abbr = TRUE,
    week_start = 1
  ),
  bow = (wday - 1) * 48 + Period,
  dom = day(Date),
  bom = (dom - 1) * 48 + Period,
  Weekend = if_else(wday %in% c(6, 7), 1, 0),
  Indx_hour = ceiling(Period / 2),
  Indx_Hour_Yr = Indx_hour + 24 * (yday - 1),
  Indx_Hour_Month = Indx_hour + 24 * (Indx_Day_Month - 1),
  Indx_Hour_Wk = Indx_hour + 24 * (wday - 1)
)




VIC <- as_tibble(VIC)
```


```{r allFig, fig.height=5.5, out.width="50%", fig.pos = "p", fig.align= 'left',echo=FALSE, eval=TRUE, fig.cap="Various probability distribution plots of electricity consumption data of Victoria from 2012 to 2014. (a) Letter value plot by DoM and MoY, (b) Decile plot by HoD and DoW (c) Box plot by DoY and DoM, (d) Violin plot of DoM and WoM, (e) Ridge plot by WoM and WoY, (f) Decile plot by DoY and MoY. Only plots (a) and (b) show harmonised time variables.", fig.show = 'hold'}

par(mfrow = c(3, 2))

VIC %>% filter(year %in% c(2012, 2013, 2014), Indx_Month %in% c(1, 2, 4, 12)) %>% ggplot(aes(Indx_Day_Month, OperationalLessIndustrial, group = Indx_Day_Month)) + geom_lv(aes(fill = ..LV..), outlier.colour = "red", outlier.shape = 1) + scale_fill_brewer() + facet_wrap(~Indx_Month) + ylab("Electricity Demand [KWh]") + xlab("Days of the Month") + scale_x_continuous(breaks = seq(0, 31, 5)) + ggtitle("(a) Letter value plot")



VIC_hod_dow <- VIC %>%
  filter(year %in% c(2012, 2013, 2014)) %>%
  group_by(wday, Indx_hour) %>%
  do({
    x <- .$OperationalLessIndustrial
    map_dfr(
      .x = seq(0.1, 0.9, 0.1),
      .f = ~ tibble(
        Quantile = .x,
        Value = quantile(x, probs = .x, na.rm = TRUE)
      )
    )
  }) %>%
  filter(wday %in% c(1, 2, 6, 7))

VIC_hod_dow %>% ggplot(aes(x = Indx_hour, y = Value, col = as.factor(Quantile))) + geom_line() + facet_wrap(~wday) + scale_x_continuous(breaks = seq(1, 24, 5)) + ylab("") + xlab("Day of the Week") + theme(legend.position = "none", strip.text = element_text(size = 7, margin = margin())) + ggtitle("(b) Decile plot")


VIC %>%
  filter(year %in% c(2012, 2013, 2014), Indx_Day_Month %in% c(1, 15, 29, 31)) %>%
  ggplot(aes(as.factor(yday), OperationalLessIndustrial, group = yday)) + geom_boxplot() + facet_wrap(~Indx_Day_Month) + ylab("Electricity Demand [KWh]") +
  xlab("Days of the Year") + scale_x_discrete(breaks = seq(0, 366, 60)) + theme(legend.position = "bottom", strip.text = element_text(size = 7, margin = margin())) + ggtitle("(c) Box plot")



VIC %>% filter(year %in% c(2012, 2013, 2014), Indx_Wk_Month %in% c(1, 2, 4)) %>% ggplot(aes(as.factor(Indx_Day_Month), OperationalLessIndustrial)) + geom_violin(alpha = 0.03) + facet_wrap(~Indx_Wk_Month, nrow = 3) + ylab("") + xlab("Days of the Month") + theme(legend.position = "bottom", strip.text = element_text(size = 7, margin = margin())) + scale_x_discrete(breaks = seq(0, 31, 5)) + scale_y_continuous(breaks = seq(2000, 9000, 2000)) + ggtitle("(d) Violin plot")


VIC %>% dplyr:::filter(year %in% c(2012, 2013, 2014), Indx_Wk_Month %in% c(1, 2, 5), Indx_Wk_Yr < 20) %>% ggplot(aes(x = OperationalLessIndustrial, y = as.factor(Indx_Wk_Yr), group = Indx_Wk_Yr)) + geom_density_ridges2() + facet_wrap(~Indx_Wk_Month) + xlab("Electricity Demand [KWh]") + ylab("Weeks of the Year") + scale_x_continuous(breaks = seq(2000, 10000, 3000)) + theme(legend.position = "bottom", strip.text = element_text(size = 7, margin = margin())) + ggtitle("(e) Ridge plot")


VIC_moy_doy <- VIC %>%
  filter(year %in% c(2012, 2013, 2014)) %>%
  group_by(Indx_Month, yday) %>%
  do({
    x <- .$OperationalLessIndustrial
    map_dfr(
      .x = seq(0.1, 0.9, 0.1),
      .f = ~ tibble(
        Quantile = .x,
        Value = quantile(x, probs = .x, na.rm = TRUE)
      )
    )
  }) %>%
  filter(Indx_Month %in% c(1, 7, 11))

VIC_moy_doy %>% ggplot(aes(x = yday, y = Value, col = as.factor(Quantile), group = yday)) + geom_line() + facet_wrap(~Indx_Month) + scale_x_continuous(breaks = seq(1, 336, 60)) + ylab("") + xlab("Day of the Year") + theme(legend.position = "none", strip.text = element_text(size = 7, margin = margin())) + ggtitle("(f) Decile plot")

# grid.arrange(p1,p2,p3,p4,nrow = 2)

#
# x <- gridExtra::arrangeGrob (p5, p6, p1, p2, p3, p4,newpage = TRUE)
# grid::grid.draw(x)

#
# gridExtra::grid.arrange(
#   p1,
#   p2,
#   p3,
#   p4,
#   p5,
#   p6,
#   nrow = 3,
#   top = "Title of the page",
#   bottom = grid::textGrob(
#     "this footnote is right-justified",
#     gp = grid::gpar(fontface = 3, fontsize = 9),
#     hjust = 1,
#     x = 1
#   )
# )
# plot.new()
# mtext('Various probability distribution plot of electricity consumption data of Victoria from 2012 to 2014 (top left) Letter Value plot by DoM and MoY (top right) Decile Plot by HoD and DoW (middle left) Box plot by DoY and DoM (middle right) Violin plot of DoM and WoM (bottom left) Ridge plot by WoM and WoY (bottom right) Decile plot by DoY and MoY', side = 3, line = - 10, outer = TRUE)
```


In \autoref{fig:allFig} (c), we have empty combinations when we plot observations with "Day-of-Month" and "Day-of-Year". Here, the 1st day of the month can never correspond to 2nd, 3rd or 4th day of the year. Hence,  "Day-of-Month" and "Day-of-Year" are clashes due to the way they map to calendar.

In \autoref{fig:allFig} (a), we will not have any empty combinations because every DoM can occur in all MoY. Here, mapping from days to months is irregular in the sense that one month can consist of 28, 29, 30 or 31 days. There is no denying that the 29th, 30th or 31st day of the month needs to be analysed with caution due to the irregular mapping, however, the first 28 days of the month will occur in all months of the year.

More generally, if we have two circular granularities C1 and C2 which has [A, B, C, D] and [X, Y, Z] categories/levels. When C1 and C2 are used as aesthetics or facets, problems arise when we encounter empty combinations. Let $S_{i,j}$ be the set of combination of the levels of C1 and C2. In this example, we have 12 such sets $S_{i,j}$ because i can take 4 values and j can take 3 values. The graphs that don't work are those where many of these 12 sets are empty. In other words, if there are levels of the x-axis which are not spanned by levels of the faceted variable or vice versa we will have structurally empty sets leading to potential ineffective graphs.

Thus, the common link that differentiates harmonies from clashes are:

  a) There should not be any levels of the faceted variable which is empty for one or more levels of the factor plotted across x-axis.
  
  b) There should not be any level of the factor plotted across x-axis which doesn't have values for all levels of factors plotted across facets.
<!-- ## Distributions conditional on bivariate granularities -->

The time series variable can be plotted against many time granularities to get more understanding of the underlying periodicty, however, we will restrict ourselves to see the distribution of the time series across bivariate temporal granularities. That neccessiates plotting one temporal granularity along the x-axis and the other one across facets. 

Now, due to the hierarchical arrangement of the granularities, there are certain granularities which when plotted together do not give us the layout to do exploration, for example, structurally empty combinations (clashes) are not recommended to plot together. The harmonies when plotted together can help exploration. But still the question remains that which distribution plot should be chosen to bring out the best of exploratory data analysis. This is a function of which features of the distribution we are interested to look at, how much display space is available to us and also if the number of observations are enough for that distribution plot.

## Advice algorithm for exploring conditional probability distributions

Recommendations for distribution plots depend on the levels(very high/high/medium/low) of the two granularities plotted. They will vary depending on which granularity is placed on the x-axis and which one across facets. Assumptions are made to ensure display is not too cluttered by the space occupied by various kinds of distribution plots. Moreover, the recommendation system ensures that there are just enough observations before choosing a distribution plot.

Levels are categorized as very high/high/medium/low each for the facet variable and the x-axis variable. Default values for these levels are based on levels of common temporal granularities like day of the month, day of a fortnight or day of a week. For example, any levels above 31 is considered as very high, any levels between 14 to 31 are taken as high and that between 7 to 14 is taken as medium and below 7 is low. 31, 14 and 7 are the levels of days-of-month, days-of-fortnight and days-of week respectively. These default values are decided based on usual cognitive power while comparing across facets and display size available to us. Let us consider case by case and see which plots are better suitable in which scenarios.

 -  very high facet and x-axis levels
 
 X-axis variable is treated as a categorical variable and hence any plots which 


# Case study: Analysis on smart meter data {#smartmeter}

Smart meters provide large quantities of measurements on energy usage for households across Australia, and indeed many parts of the world. Households are distributed geographically and have different demographic properties such as the existence of solar panels, central heating or air conditioning. The behavioral patterns in households vary substantially, for example, some families use a dryer for their clothes while others hang them on a line, and some households might consist of night owls, while others are morning larks. 

<!-- Existing approaches, why we need to drill down, or why we want to -- probability distributions -->
It is common to see aggregates of usage across households, total kwh used each half-hour by state, for example, because energy companies need to understand maximum loads that they will have to plan ahead to accommodate. But studying overall energy use hides the distributions of usage at finer scales, and making it more difficult to find solutions to improve energy efficiency. 


One of the customer trial [@smart-meter] conducted as part of the Smart Grid Smart City (SGSC) project (2010-2014) in Newcastle, New South Wales and some parts of Sydney provides customer wise data on half-hourly energy usage and detailed information on appliance use, climate, retail and distributor product offers, and other related factors. It would be interesting to explore the energy consumption distribution for these customers and gain more insights on their energy behavior which are otherwise lost either due to aggregation or looking only at coarser temporal units. The idea here is to show how looking at the time across different granularities together can help identify different behavioral patterns.

Let us see the behavior of typical and extreme behaviors of 50 households of this trial. We want to  look at the distribution of energy across coarser temporal granularities and then deep dive into finer temporal granularities.


```{r search_gran}

library(gravitas)
library(tsibble)
load("data/sm_cust50.RData")


library(ggplot2)
sm_cust50 %>%
  granplot("month_year", "hour_day",
    response = "general_supply_kwh",
    plot_type = "quantile",
    quantile_prob = c(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)
  ) +
  ggtitle("Quantile plot across hour-of-day by month-ofyear") + ggtitle("") +
  scale_x_discrete(breaks = seq(0, 23, 2)) + scale_color_brewer(type = "div",
                                                                palette = "Dark2", direction = -1)

#observation: the two peaks of the day are more ditinct in winter months compared to summaer months, where the energy profile is quite flat.

# Hourly usage across days of the week
sm_cust50 %>%
  granplot("day_week", 
           "hour_day",
           "general_supply_kwh", 
           plot_type = "quantile", 
          quantile_prob = c(0.25, 0.5, 0.75),
           response = "general_supply_kwh") +
  ggtitle("") +
  ggtitle("Quantile plot across hour-of-day and day-of-week") +
  scale_x_discrete(breaks = seq(0, 23, 3)) +
  scale_color_brewer(type = "div",
                     palette = "Dark2", direction = -1)
#observation: on saturday and sunday morning peaks are higher and little late (in absolte time) than other weekdays


# Across days of the week by months
sm_cust50 %>%
  granplot("month_year",
    "day_week",
    response = "general_supply_kwh", plot_type = "quantile", 
          quantile_prob = c(0.25, 0.5, 0.75)) + ggtitle("") +
  ggtitle("Quantile plot across day-of-week by month-of-year") 

# Across week of the month and weekends

sm_cust50 %>%
  dynamic_create_gran("day_week") %>%
  filter(day_week %in% c("Sat", "Sun")) %>%
  granplot("day_week", "week_month",
           response = "general_supply_kwh",
           plot_type = "lv") + 
  ggtitle("") +
  ggtitle("Letter value plot across day-of-week by week-of-month") +
  scale_y_continuous(breaks = seq(0, 1.5, 0.25))


sm_cust50 %>%
  granplot("fortnight_month", "hour_day",
           response = "general_supply_kwh",
           plot_type = "lv") + 
  ggtitle("")
```





# Case study: Analysis on cricket {#cricket}

Th application is not only restricted to temporal data. We provide an example of cricket to illustrate how this can be generalised in other applications. The Indian Premier League (IPL) is a professional Twenty20 cricket league in India contested by eight teams representing eight different cities in India. With eight teams, each team plays each other twice in a home-and-away round-robin format in the league phase. In a Twenty20 game the two teams have a single innings each, which is restricted to a maximum of 20 overs. Hence, in this format of cricket, a match will consist of 2 innings, an innings will consist of 20 overs, an over will consist of 6 balls. We have sourced the the ball by ball data for IPL 2008 from https://www.kaggle.com/littleraj30/indian-premier-league-2019-ball-by-ball. There are many interesting questions that can possibly be answered with such a dataset, however, we will explore a few and understand how the proposed approach in the paper can help answer some of the questions.


The dataset contains the information on match_id, inning, batting team, bowling team, over of the innings, balls of the over, total runs and many more. A hierarchy table like [] can be construed for this game format:

```{r hierarchy, echo=FALSE}
hierarchy_model <- tibble::tibble(units = c("index", "ball", "over", "inning", "match"), 
                                  convert_fct = c(1, 6, 20, 2, 1))
knitr::kable(hierarchy_model, caption = "T20 hierarchy table")
```


Each team is given a two-and-a-half-minute "strategic timeout" during each innings; one must be taken by the bowling team between the ends of the 6th and 9th overs, and one by the batting team between the ends of the 13th and 16th overs.

Suppose, we are interested to see how the distribution of scores vary from the start to the end of the game. Let us brainstorm some of the questions that might help us comprehend that.

a) What is the most common pattern for batting and bowling teams across balls, overs, innings and matches? Which are the teams which are typical and which are exceptions?

b) How the scores vary per each over each periodic granularities like ball of the over, ball of the innings, over of the inning, over of the match, inning of the match and others.
 
We will look at the ball by ball data for all batting teams.Since we want a periodic world, where each over consists of 6 balls and each match consists of two innings, we shall filter out the matches or overs for which that is not true.

```{r read_data, echo=FALSE}

cricket_tsibble <- cricketdata %>%
  mutate(data_index = row_number()) %>%
  as_tsibble(index = data_index)

# cricket_tsibble %>% dynamic_create_gran(
#   "ball_over",
#   hierarchy_model
# )

#over_inning    ball_over 
cricket_tsibble %>% granplot("over_inning", "ball_over",
  hierarchy_model,
  response = "total_runs",
  plot_type = "quantile",
  quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)
  
  
)

#inning_match   ball_over 

cricket_tsibble %>% granplot("inning_match", "ball_over",
  hierarchy_model,
  response = "total_runs",
   plot_type = "quantile",
  quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)
)

# inning_match   ball_inning 
#interesting plot
cricket_tsibble %>% granplot("inning_match", "ball_inning",
  hierarchy_model,
  response = "total_runs",
   plot_type = "quantile",
  quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9))+
  scale_x_discrete(breaks = seq(1,120,5))



# ball_over      over_inning  

cricket_tsibble %>% granplot("ball_over", "over_inning",
  hierarchy_model,
  response = "total_runs",
  plot_type = "quantile",
  quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9))


# inning_match   over_inning  

cricket_tsibble %>% granplot("inning_match", "over_inning",
  hierarchy_model,
  response = "total_runs"
)

# ball_over      over_match  

cricket_tsibble %>% granplot("ball_over", "over_match",
  hierarchy_model,
  response = "total_runs"
)

# ball_over      inning_match  

cricket_tsibble %>% granplot("ball_over", "inning_match",
  hierarchy_model,
  response = "total_runs"
) + scale_color_brewer(type = "div", palette = "Dark2")


cricket_tsibble %>%
  granplot(
    gran1 = "inning_match",
    gran2 = "over_inning",
    hierarchy_model,
    response = "total_runs",
    plot_type = "quantile"
  )


```


To see more variation in the response variable, we can also see the distribution of runs per over.

```{r runs_per-over}
cricket_per_over <- cricketdata %>% 
  group_by(match_id, batting_team, bowling_team,  inning, over) %>%
  summarise(runs_per_over = sum(total_runs)) 

cricket_tsibble <- cricket_per_over %>% 
  ungroup() %>% 
  mutate(data_index = row_number()) %>% 
  as_tsibble(index = data_index)

  
  cricket_tsibble %>% granplot("inning_match", "over_inning",
                               hierarchy_model,
                               response = "runs_per_over",
                               plot_type = "quantile",
                               quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)
                               
                               
  )

  
  cricket_tsibble %>%
    filter(batting_team == "Chennai Super Kings") %>% 
    granplot("inning_match", "over_inning",
                               hierarchy_model,
                               response = "runs_per_over",
                               plot_type = "quantile",
                               quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)
  )


    cricket_tsibble %>%
    filter(batting_team == "Kolkata Knight Riders") %>% 
    granplot("inning_match", "over_inning",
                               hierarchy_model,
                               response = "runs_per_over",
                               plot_type = "quantile",
                               quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9)
  )


```





# Discussion {#sec:discussion}


# Acknowledgements {-}


# Bibliography

<!-- ```{r write-bib} -->
<!-- write_bib(c("ggplot2", "tidyverse", "sugrrants", "geofacet", "shiny", "wanderer4melb"), 'packages.bib') -->
<!-- ``` -->
