---
title: Exploring probability distributions for bivariate temporal granularities
authors:
- name: Sayani Gupta
  affiliation: Department of Econometrics and Business Statistics, Monash University
  email: earo.wang@monash.edu

- name: Rob J Hyndman
  affiliation: Department of Econometrics and Business Statistics, Monash University

- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University

keywords:
- data visualization
- statistical distributions
- time granularities
- periodicties
- grammar of graphics
- R

abstract: |
  Smart meters measure energy usage at fine temporal scales, and are now installed in many households around the world. We propose some new tools to explore this type of data, which deconstruct time in many different ways. There are several classes of time deconstructions including linear time granularities, circular time granularities and aperiodic calendar categorizations. Linear time granularities respect the linear progression of time such as hours, days, weeks and months. Circular time granularities accommodate periodicities in time such as hour of the day, and day of the week. Aperiodic calendar categorizations are neither linear nor circular, such as day of the month or public holidays.

  The hierarchical structure of many granularities creates a natural nested ordering. For example, hours are nested within days, days within weeks, weeks within months, and so on. We refer to granularities which are nested within multiple levels as "multiple-order-up" granularities. For example, hour of the week and second of the hour are both multiple-order-up, while hour of the day and second of the minute are single-order-up.

   Visualizing data across various granularities helps us to understand periodicities, pattern and anomalies in the data. Because of the large volume of data available, using displays of probability distributions conditional on one or more granularities is a potentially useful approach. This work provides tools for creating granularities and exploring the associated within the tidy workflow, so that probability distributions can be examined using the range of graphics available in the [ggplot2](https://cran.r-project.org/package=ggplot2) package. 
bibliography: bibliography.bib
output:
  bookdown::pdf_book:
    #base_format: rticles::asa_article
    fig_height: 5
    fig_width: 8
    dev: "pdf"
---

```{r initial, echo = FALSE, cache = FALSE, include = FALSE}
options("knitr.graphics.auto_pdf" = TRUE)
library(knitr)
opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE, comment = "#>",
  fig.path = 'figure/', fig.align = 'center', fig.show = 'hold', 
  cache = FALSE, cache.path = 'cache/', 
  out.width = ifelse(is_html_output(), "100%", "\\textwidth")
)
knitr::opts_knit$set(root.dir = here::here())
```

```{r external, include = FALSE}
#read_chunk('scripts/main.R')
```

```{r load}
```

# Introduction

<!--temporal granularities and why should we care --> 
Temporal data can be available at various resolution depending on the context. Social and economic data are often collected and reported at coarses temporal scales like monthly, quarterly or annually. But with recent advancement in technology, more and more data are recorded and stored at much finer temporal scales than that was previously possible. For example, it might be sufficient to observe energy consumption every half an hour, but energy supply needs to be monitored every minute and number of web searches requires optimisation every second.  As the frequency of data increases, the number of questions about the observed variable that need to be addressed also increases. For example, data collected at an hourly scale can be analyzed using coarser temporal scales like days, months or quarters. This approach requires deconstructing time in various possible ways. 

A temporal granularity which results from such a deconstruction may be intuitively described as a sequence of time granules, each one consisting of a set of time instants. There are several classes of time deconstructions including linear time granularities, circular time granularities and aperiodic calendar categorizations. Linear time granularities respect the linear progression of time such as hours, days, weeks and months. Circular time granularities accommodate periodicities in time such as hour of the day, and day of the week. Aperiodic calendar categorizations are neither linear nor circular, such as day of the month or public holidays.

It is important to be able to navigate through all of these temporal granularities to have multiple perspectives on the observed data. This idea aligns with the notion of EDA [@Tukey1977-jx] which emphasizes the use of multiple perspectives on data to help formulate hypotheses before proceeding to hypothesis testing. 


<!--motivation --> 

The motivation for this work comes from the desire to provide methods to better understand large quantities of measurements on energy usage reported by smart meters in household across Australia, and indeed many parts of the world. Smart meters currently provide half-hourly use in kwh for each household, from the time that they were installed, some as early as 2012. Households are distributed geographically, and have different demographic properties such as the existence of solar panels, central heating or air conditioning. The behavioral patterns in households vary substantially, for example, some families use a dryer for their clothes while others hang them on a line, and some households might consist of night owls, while others are morning larks. 

It is common to see aggregates of usage across households, total kwh used each half hour by state, for example, because energy companies need to understand maximum loads that they will have to plan ahead to accommodate. But studying overall energy use hides the distributions of usage at finer scales, and making it more difficult to find solutions to improve energy efficiency. 

We propose that the analysis of probability distributions of smart meter data at finer or coarser scales can be benefited from the approach of Exploratory Data Analysis (EDA). EDA calls for utilizing visualization and transformation to explore data systematically. It is a process of generating hypothesis, testing them and consequently refining them through investigations.

<!--details abt time gran --> 
The hierarchical structure of many granularities creates a natural nested ordering. For example, hours are nested within days, days within weeks, weeks within months, and so on. We refer to granularities which are nested within multiple levels as "multiple-order-up" granularities. For example, hour of the week and second of the hour are both multiple-order-up, while hour of the day and second of the minute are single-order-up. 

<!--existing closely related work and how ours relate --> 
[**lubridate**]([https://cran.r-project.org/package=lubridate)#acad paper in R allows creation of granularities that are mostly single-order-up like hour of the day, second of the minute. This paper utilises the nestedness of time granularities to obtain multiple-order-up granularities from single-order-up ones.

<!--actions on multiple order up - usage --> 
Finally, visualizing data across single/multiple order-up granularities help us to understand periodicities, pattern and anomalies in the data. Because of the large volume of data available, using displays of probability distributions conditional on one or more granularities is a potentially useful approach. However, this approach can lead to a myriad of choices all of which are not useful. Analysts are expected to iteratively visualise these choices for exploring possible patterns in the data. But too many choices might leave him bewildered. 

<!-- challenges and how you are dealing wih it --> 
This work provides tools for systematically exploring bivariate granularities within the tidy workflow through proper study of what can be considered a prospective graphic for exploration. Pairs of granularities are categorized as either a *harmony* or *clash*, where harmonies are pairs of granularities that aid exploratory data analysis, and clashes are pairs that are incompatible with each other for exploratory analysis. Probability distributions can be examined using the range of graphics available in the [ggplot2](https://cran.r-project.org/package=ggplot2) package. 


In particular, this work provides the following tools.

  * Functions to create multiple-order-up time granularities. This is an extension to the [lubridate]([https://cran.r-project.org/package=lubridate) package, which allows for the creation of some calendar categorizations, usually single-order-up.

  * Checks on the feasibility of creating plots or drawing inferences from two granularities together. Pairs of granularities can be categorized as either a *harmony* or *clash*, where harmonies are pairs of granularities that aid exploratory data analysis, and clashes are pairs that are incompatible with each other for exploratory analysis.

<!-- The remainder of the paper is organized as follows. Section \@ref(sec:algorithm) details the construction of the calendar layout in depth. It describes the algorithms of data transformation (Section \@ref(sec:transformation)), the available options (Section \@ref(sec:opt)), and variations of its usage (Section \@ref(sec:variations)). Section \@ref(sec:facet-calendar) explains the full faceting extension, that is equipped with formal labels and axes. An analysis of half-hourly household energy consumption, using the calendar display, is illustrated in a case study in Section \@ref(sec:case). Section \@ref(sec:discussion) discusses the limitations of calendar displays and possible new directions. -->

# Time granularities{#sec:algorithm}


Time can be represented at varied levels of abstraction depending on the accuracy required for the context. Time granularity can be defined as the resolution power of the temporal qualification of a statement. Providing a formalism with the concept of time granularity is important to model time information across differently grained temporal domains{#Euzenat2005-de}.

## Linear time granularities{#linear-gran-def}

Linear granularities are defined in @Bettini1998-ed, as follows.

**Definition:** A **time domain** is a pair $(T; \le)$ where $T$ is a non-empty set of time instants and $\le$ is a total order on $T$.

A time domain can be **discrete** (if there is unique predecessor and successor for every element except for the first and last one in the time domain), or it can be **dense** (if it is an infinite set). A time domain is assumed to be discrete for the purpose of our discussion.

**Definition:** A linear **granularity** is a mapping $G$ from the integers (the index set) to subsets of the time domain such that:

  (C1) if $i < j$ and $G(i)$ and $G(j)$ are non-empty, then each element of $G(i)$ is less
than all elements of $G(j)$, and  
  (C2) if $i < k < j$ and $G(i)$ and $G(j)$ are non-empty, then $G(k)$ is non-empty.  

**Definition:** Each non-empty subset $G(i)$ is called a **granule**, where $i$ is one of the indexes and $G$ is a linear granularity.


The first condition implies that the granules in a linear granularity are non-overlapping and their index order is same as time order.


<!--need to include this \autoref{fig:linear-time} shows the implication of this condition. If we consider the bottom linear granularity [@aigner2011visualization] as hourly and the entire horizon has T hours then it will have $\lfloor T/24\rfloor$ days, $\lfloor s/(24*7)\rfloor$ weeks and so on. -->

<!-- ```{r linear-time,echo=FALSE,out.width = "100%",fig.cap="\\label{fig:linear-time} The time domain distributed as linear granularities"} -->
<!-- knitr::include_graphics("Figs/linear-time.png") -->
<!-- ``` -->

## Formal conceptualisation of circular time granularities{#cirgran-def}

Suppose we have a tsibble [@Wang2019-aw] with a time index in one column and keys and variables in other columns.  A time domain, as defined by Bettini, is essentially a mapping of row numbers (the index set) to the time index.  A linear granularity is a mapping of row numbers to subsets of the time domain. For example, if the time index is days, then a linear granularity might be weeks, months or years.

For circular granularities, we need a symbolic representation of time to represent periodicity, which we call 'calendar categorization'. Anything that maps a time index to a categorical variable can be considered a calendar categorization. The number of categories is essentially the periodicity of a circular time granularity.

We want to use modular arithmetic on the domain of the circular granularity to define the calendar categorization. Hence, we start with the definition of equivalence classes and then move on to define a circular granularity. 

**Definition: Equivalence class** Let $m \in N \backslash{0}$. For any $a \in Z$ (set of integers), $[a]$ = {$b \in Z | a \equiv (b \mod m)$} where $[a]$ is defined as the equivalence class to which a belongs.

The set of all equivalence classes of the integers for a modulus $m$ is called the ring of integers modulo $m$, denoted by $Z_m$. Thus $Z_m = \{[0], [1], ..., [m-1]\}$. However, we often write $Z_m = \{0, 1, ..., (m-1)\}$, which is the set of integers modulo $m$.

<!-- A **calendar domain** is a triplet $(T; \le; M, \le)$ where $T$ is a non-empty set of time instants, m is a modular period, M is the ring of integers modulo m denoted by the set ${1,2,\dot,(m-1)}$ $\le$ is a total order on the set $M$ and   -->

<!-- (Notes: a totally ordered set requires that every element in the set is comparable) -->

<!-- A **calendar domain** is a pair $(T; \le)$ where $T$ is a non-empty set of time instants and $\le$ is a **total** order on $T$. -->

**Definition:** A **circular granularity** $C$ with a modular period m is defined to be a mapping from the integers $Z$ (Index Set) to $Z_m$, such that $C(s) = (s\mod m)$ for $s \in Z$.

For example, suppose $C$ is a circular granularity denoting Hour-of-Day and we have hourly data for 100 hours. The modular period $m = 24$, since each day consists of 24 hours and $C$ is a mapping from ${1,2,\dots, 100}$ to ${0,1,2, \dots, 23}$ such that  $C(s)= s \mod 24$ for $s \in {1,2,\dots, 100}$.


**Definition:** A **cycle** is defined as the progression of each circular granularity with modular period m through {$1,2,\dots,(m-1),0$} once.

**Definition:** A **circular granule** represents an equivalence class inside each cycle.


A few categorizations are listed below using modular arithmetic with appropriate period to illustrate further. We assume that the time index is in hours, and $n_i$ is the number of categories created by $C_i$. Then the following categorizations can be computed.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lr@{~}lr@{~}r}
\toprule
HOD: & $C_1(s)$ & $= s \mod 24$ & $n_1 =$&$24$ \\
HOW: & $C_2(s)$ & $= s \mod `r 24*7`$ & $n_2=$&$`r 24*7`$\\
HOM: & $C_3(s)$ & $= s \mod `r 24*30`$ (approximately) & $n_3=$&$`r 24*31`$\\
HOY: & $C_4(s)$ & $= s \mod `r 24*365`$ (except for leap years) & $n_4=$&$`r 24*366`$\\
DOW: & $C_5(s)$ & $= \lfloor s/24\rfloor \mod 7$ & $n_5=$&$7$\\
DOM: & $C_6(s)$ & $= \lfloor s/24\rfloor\mod 30$  (approximately) & $n_6=$&$31$\\
DOY: & $C_7(s)$ & $= \lfloor s/24\rfloor \mod 365$ (except for leap years) & $n_7=$&$366$\\
WOM: & $C_8(s)$ & $=\lfloor s/168 \rfloor \mod 4$ (approximately) & $n_8 =$&$ 5$\\
WOY: & $C_9(s)$ & $=\lfloor s/168\rfloor \mod 52$ (approximately) & $n_9 =$&$ 53$\\
MOY: & $C_{10}(s)$ & $= \lfloor s/720\rfloor \mod 12$ (approximately) & $n_{10}=$&$12$\\
\bottomrule
\end{tabular}
\end{center}
\caption{Illustrative calendar categorizations with hourly time index}
\label{definitions}
\end{table}

Note that most of the formulas are approximations only due to unequal month and year lengths, and due to the fact that there are not an integer number of weeks per month or weeks per year.

## Computing multiple order-up granularities from single order-up granularities recursively

<!-- Often sets come equipped with a natural hierarchical structure.  For example, the set of natural numbers N is equipped with a natural pre-order structure, where $n\leq n'$ whenever we can find some other number m so that $n+m=n'$. Similarly, -->

The hierarchical structure of time creates a natural nested ordering, where hours are nested within days, days within weeks, weeks within months, and so on. We refer to granularities which are nested within multiple levels as "multiple-order-up" granularities. For example, hour of the week and second of the hour are both multiple-order-up, while hour of the day and second of the minute are single-order-up.

Consider a hierarchy table laying out the structure of all temporal granularities we are interested to look at, consisting of the following  three columns: 
 - The first column represents the temporal granularities in ascending order.   
 - The second column represents the constant which relates subsequent granularities.  
 - The third column provides the functional form for single-order up-granularities.  
 
Order: Order is defined as the position of the granularity in the hierarchical table.
If the two granularties have three intermediate granularities in the hierarchical table, then their order difference is 3.

Then it is possible to obtain all multiple-order-up granularities from this hierarchy table using a recursive relation.

Suppose, for $order(x) <  order(y)$ \\
$f(x, y)$ represents the functional form for computing granularity x_y
$c(x, y)$ is a constant which relates x and y. 

Then, 
$f(x, y) & = f(x, x+1) + c(x, x+1)*[f(x+1, y)-1] $, for for $order(x+1) < order(y)$ can be used recursively to obtain any multiple order-up granularities. For $order(x+1) =  order(y)$, the function is same as the single-order-up granularities.

\begin{equation} \label{eq1}
\begin{split}
f(x, y) & = f(x, x+1) + c(x, x+1)*[f(x+1, y)-1] \\  
  & =  f(x, x+1) + c(x, x+1)[f(x+1, x+2) + c(x+1, x+2)[f(x+2, y) - 1] - 1]\\ (expanding f(x+1, y))  
  & =  f(x, x+1) + c(x, x+1)[f(x+1, x+2) - 1] + c(x, x+1)c(x+1, x+2)[f(x+2, y) - 1]
\end{split}
\end{equation}





# Obtaining multiple-order-up granularities from just granularity which relates the lowest temporal unit to the highest one in the hierarchical table.

Suppose, we are not given the intermediate single-order-up granularities but still want all possible multiple order-up granularties. In that case, it is necessary to atleast have the have the functional form that relates the smallest temporal unit to the highest temporal unit. In that case, we are able to form single-order-up from this functional relationship and still use the recursive relation to get the multiple order-up granularities.

  In the above equations, it is shown how to obtain f(x,z) given f(x,y) and the hierarchy table,  provided $order(z) > order(y) \neq order(x)$. In general,
  
  $f(x,z) = \sum_{i=0}^{order(z) - order(y) + 1}c(x, x+i)f(x+1, x+i)$
    
  We could also derive f(w, y) given f(x, y) and the hierarchy table, for $order(y)>=order(w) > order(x).
  
  $f(w,y) = f(x,y) \mod c(x,w)$, for $f(x,y) \mod c(x,w)!=0$ \\
  $= c(x,w))$, otherwise.

Let us investigate if that is true:
  
f(ball, over) = 1
f(over, quarter) = 4
f(quarter, semester) = 2
f(semester, match) = `1

then, f(ball_quarter) = 1 + 6*4 = 25, 
f(ball, semester) = 1 + 6*4 + 5*6*1 = 55
f(ball, semester) = 1 + 6*4 + 5*6*1 + 5*6*2*1 = 115


# Harmony and Clashes

We investigate some combinations of circular time granularities which facilitate or hinder exploratory analysis. The combinations of circular granularities which promote the exploratory analysis through visualization are referred to as **harmonies** and the ones which impede the analysis are referred to as **clashes**.

Let's take a specific example, where $C_1$ maps row numbers to Day-of-Month and $C_2$ maps row numbers to Week-of-Month. Here $C_1$ can take 31 values while $C_2$ can take 5 values. There will be $31\times 5=155$ sets $S_{ij}$ corresponding to the possible combinations of WOM and DOM. Many of these are empty. For example $S_{1,5}$, $S_{21,2}$, etc. In fact, most of these 155 sets will be empty, making the combination of $C_1$ and $C_2$ in a graph unhelpful. These are structurally empty sets in that it is impossible for them to have any observations.

Another example could be where $C_1$ maps row numbers to Day-of-Week and $C_2$ maps row numbers to Month-of-Year. Here $C_1$ can take 7 values while $C_2$ can take 12 values. So there are $12\times7=84$ sets $S_{ij}$ corresponding to the possible combinations of DOW and MOY. All of these are non-empty because every DOW can occur in every month. So graphics involving $C_1$ and $C_2$ aare potentially useful.



# Visualisation

<!-- Why visualise -->
Analysts often want to fit their data to statistical models, either to test hypotheses or predict future values. However, improper choice of models can lead to wrong predictions. One important use of visualization is exploratory data analysis, which is gaining insight into how data is distributed to inform data transformation and modeling decisions.

<!-- When the data for entire population is available at our disposal, then there is no inference that is to be made for the unobserved population. We can just examine what is in front of us. -->


<!-- Why distribution plot -->
<!-- Huge data difficult to summarise using one statistic-->
But with huge amount of data being available, sometimes mean, median or any one summary statistic is not enough to understand a dataset. Soon enough following questions like below become more interesting:  

 - Are values clustered around mean/median or mostly around tails? In other words, what is the combined weight of tails relative to the rest of the distribution? 
 
 - Does values rise very quickly between 25th percentile and median but not as quickly between median and 75th percentile? More generally, how the variation is the dataset changes across different percentiles/deciles?
 
  - Is the tail on the left hand side longer than that on the right side? Or are they equally balanced around mean/median?
 
This is when displaying a probability distribution becomes a potentially useful approach.


<!-- What all can you see from a distribution plot-->

The entire distribution can be visualized or some contextual summary statistics can be visualised to emphasize certain properties of the distribution. These properties can throw light on central tendency, skewness, kurtosis, variation of the distribution and can also be useful in detecting extreme behavior or anolmalies in the dataset.


<!-- Lit review on statistical distribution plot-->
Most commonly used techniques to display distribution of data include the histogram (Karl Pearson), which shows the prevalence of values grouped into bins and the box-and-whisker plots [@Tukey1977-jx] which convey statistical features such as the median, quartile boundaries, hinges, whiskers and extreme outliers. The box plot is a compact distributional summary, displaying less detail than a histogram. Due to wide spread popularity and simplicity in implementation, a number of variations are proposed to the original one which provides alternate definitions of quantiles, whiskers, fences and outliers. Notched box plots [@Mcgill1978-hg, 1978] has box widths proportional to the number of points in the group and display confidence interval around medians aims to overcome some drawbacks of box plots.

The vase plot [@Benjamini1988-io, 1988] was a major revision from the concept of box plots where the width of box at each point is proportional to estimated density. Violin plots [@Hintze1998-zi, 1998] display the density for all data points and not only the box. The summary plot [@Potter2010-qc, 2010] combines a minimal box plot with glyphs representing the first five moments (mean, standard deviation, skewness, kurtosis and tailings), and a sectioned density plot crossed with a violin plot (both color and width are mapped to estimated density), and an overlay of a reference distribution. The highest density region (HDR) box plot proposed by [@Hyndman1996-ft] displays a probability density region that contains points of relatively highest density. The probabilities for which the summarization is required can be chosen based on the requirement. These regions do not need to be contiguous and help identify multi-modality. The letter-value box plot [@Hofmann2017-sg, 2006] was designed to adjust for number of outliers proportional to the data size and display more reliable estimates of tail. Because this display just adds extra letter values, it suffers from the same problems as the original box plot, and multimodality is almost impossible to spot[@Wickham_undated-vr].

Finally, a density plot which uses a kernel density estimate to show the probability density function of the variable can show the entire distribution. Also, a Ridge line plot (sometimes called Joy plot) shows the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.



Which ditribution plot to draw, is based on what features of the distribution we are interested to look at, or how much display space is available to us.

Offcourse, the time series variable can be plotted against many time granularities to get more understanding of the underlying periodicty, however, we will restrict ourselves to see the distribution of the time series across bivariate temporal granularities. That neccessiates plotting temporal granularity along the x-axis and the other one across facets. 

Now, due to the hierarchical arrangement of the granularities, there are certain granularities which when plotted together do not give us the layout to do exploration.
The harmonies when plotted together can help exploration, however, the clashes were plotted against each other is not appropriate for exploration.


We investigate some combinations of circular time granularities which facilitate or hinder exploratory analysis. Since we are restricting ourselves to 2D space, we assume that the circular granularities are plotted across the aesthetics and facets. The combinations of circular granularities which promote the exploratory analysis through visualization are referred to as **harmonies** and the ones which impede the analysis are referred to as **clashes**.


# Case study: Analysis on smart meter data {#smartmeter}

# Case study: Analysis on cricket {#cricket}

# Discussion {#sec:discussion}


# Acknowledgements {-}

We would like to thank Stuart Lee and Heike Hofmann for their feedback on earlier versions of this work. We thank Thomas Lin Pedersen for pointing out some critical **ggplot2** internals, which makes the `facet_calendar()` implementation possible. We are very grateful to anonymous reviewers for helpful comments that have led to many improvements of the paper. The **sugrrants** R package is available from CRAN <https://CRAN.R-project.org/package=sugrrants> and the development version is available on Github <https://github.com/earowang/sugrrants>. All materials required to reproduce this article and a history of the changes can be found at the project's Github repository <https://github.com/earowang/paper-calendar-vis>.

# Bibliography

<!-- ```{r write-bib} -->
<!-- write_bib(c("ggplot2", "tidyverse", "sugrrants", "geofacet", "shiny", "wanderer4melb"), 'packages.bib') -->
<!-- ``` -->